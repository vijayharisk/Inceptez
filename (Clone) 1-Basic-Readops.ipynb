{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4fad378f-70c9-41b6-9f64-1ca4e9f56a7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### First Let's understand the basic Catalog + Volume Feature of Databricks \n",
    "Filesystem Hierarchy of Volume in Databricks (DBFS)?<br>\n",
    "Catalog -> /OurWorkspace/catalog/schema(database)/volume/folder/data files<br>\n",
    "Tables Hierarchy of Databricks?<br>\n",
    "Catalog -> /OurWorkspace/catalog/schema(database)/tables/data(dbfs filesystem/some other filesystems)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a792120-045d-4a8e-be46-186a83ab50a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create catalog if not exists catalog1_dropme;\n",
    "create database if not exists catalog1_dropme.schema1_dropme;\n",
    "create volume if not exists catalog1_dropme.schema1_dropme.volume1_dropme;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a165bfbc-4536-406f-acef-c3dfc34f5e8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.mkdirs(\"/Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/directory_dropme\")\n",
    "#Upload the drive data into this location..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70c65b88-7d82-4c95-974e-c3090a4c50cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####If we need to create schema/volume/folder programatically, follow the below steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac3d7da6-f726-4414-acb9-f2ae80f7d523",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Spark SQL<br>\n",
    "###1.E(Extract) \n",
    "L(Load)<br>\n",
    "Inbuilt libraries sources/targets & Inbuilt data Formats<br>\n",
    "2. Bread & Butter (T(Transformation) A(Analytical))<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42cfb561-e32a-428d-9606-0c0981559c13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.mkdirs(\"/Volumes/workspace/wd36schema2/volume1/folder1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ddd4e1e1-467d-421d-86c6-ab5c26c4a72a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Learn How to Create Dataframes from filesystem using different options\n",
    "Download the data from the below drive url <br>\n",
    "https://drive.google.com/drive/folders/1Tw7V9eBtUxy0xQMW38z3-bzWI_ewzLm6?usp=drive_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba0588c8-0035-40ac-aa91-17479e4199af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###How Do We Write a Typical Spark Application (Core(Obsolute),SQL(Important),Streaming(Mid level important))\n",
    "####Before we Create Dataframe/RDD, what is the prerequisite? We need to create spark session object by instantiating sparksession class (by default databricks did that if you create a notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "381a6f5a-f856-4afb-ab1c-a1cb49d5dee8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "print(spark)#already instantiated by databricks\n",
    "spark1=SparkSession.builder.getOrCreate()\n",
    "print(spark1)#we instantiated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "220a856e-eeb5-4afd-8355-a1b4bdb1c27d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Create a DBFS volume namely commondata and upload the above data in that volume\n",
    "What are other FS uri's available? file:///, hdfs:///, dbfs:///, gs:///, s3:///, adls:///, blob:///"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da503d1c-553a-4eac-9b3c-6e57527269d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####How to Read/Extract the data from the filesytem and load it into the distributed memory for further processing/load - using diffent methodologies/options from different sources(fs & db) and different builtin formats (csv/json/orc/parquet/delta/tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d87bdfec-949a-4bff-9d14-ebfdf897aac8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#If I don't use any options in this csv function, what is the default functionality?\n",
    "#1. By default it will consider ',' as a delimiter (sep='~')\n",
    "#2. By default it will use _c0,_c1..._cn it will apply as column headers (header=True or toDF(\"\",\"\",\"\") or we have more options to see further)\n",
    "#3. By default it will treat all columns as string (inferSchema=True or we have more options to see further)\n",
    "csv_df1=spark.read.csv(\"dbfs:///Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/directory_dropme/custs_1\")\n",
    "#csv_df1.show(2)#display with produce output in a dataframe format\n",
    "print(csv_df1.printSchema())\n",
    "display(csv_df1)#display with produce output in a beautified table format, specific to databricks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "468329e5-2514-4c25-93b2-316ae03edef5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "csv_df1=spark.read.csv\n",
    "(path: PathOrPaths, schema: Optional[Union[StructType, str]]=None, sep: Optional[str]=None, encoding: Optional[str]=None, quote: Optional[str]=None, escape: Optional[str]=None, comment: Optional[str]=None, header: Optional[Union[bool, str]]=None, inferSchema: Optional[Union[bool, str]]=None, ignoreLeadingWhiteSpace: Optional[Union[bool, str]]=None, ignoreTrailingWhiteSpace: Optional[Union[bool, str]]=None, nullValue: Optional[str]=None, nanValue: Optional[str]=None, positiveInf: Optional[str]=None, negativeInf: Optional[str]=None, dateFormat: Optional[str]=None, timestampFormat: Optional[str]=None, maxColumns: Optional[Union[int, str]]=None, maxCharsPerColumn: Optional[Union[int, str]]=None, maxMalformedLogPerPartition: Optional[Union[int, str]]=None, mode: Optional[str]=None, columnNameOfCorruptRecord: Optional[str]=None, multiLine: Optional[Union[bool, str]]=None, charToEscapeQuoteEscaping: Optional[str]=None, samplingRatio: Optional[Union[float, str]]=None, enforceSchema: Optional[Union[bool, str]]=None, emptyValue: Optional[str]=None, locale: Optional[str]=None, lineSep: Optional[str]=None, pathGlobFilter: Optional[Union[bool, str]]=None, recursiveFileLookup: Optional[Union[bool, str]]=None, modifiedBefore: Optional[Union[bool, str]]=None, modifiedAfter: Optional[Union[bool, str]]=None, unescapedQuoteHandling: Optional[str]=None) -> \"DataFrame\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "975feb5e-4bc6-423c-9ad2-b802de601fe4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Sample data of custs_1\n",
    "4000001,Kristina,Chung,55,Pilot<br>\n",
    "4000002,Paige,Chen,77,Teacher\n",
    "\n",
    "Sample data of custs_header<br>\n",
    "custid,fname,lname,age,profession<br>\n",
    "4000001,Kristina,Chung,55,Pilot<br>\n",
    "4000002,Paige,Chen,77,Teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef7cdbf2-6f01-45a7-9dd0-9f3afad99be3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1. Header Concepts (Either we have define the column names or we have to use the column names from the data)\n",
    "#Important option is...\n",
    "#By default it will use _c0,_c1..._cn it will apply as column headers, but we are asking spark to take the first row as header and not as a data?\n",
    "csv_df1=spark.read.csv(\"/Volumes/workspace/wd36schema/ingestion_volume/source/custs_header\",header=True)\n",
    "print(csv_df1.printSchema())\n",
    "csv_df1.show(2)\n",
    "#csv_df1.write.csv(\"/Volumes/workspace/wd36schema2/volume1/folder1/outputdata\")\n",
    "#By default it will use _c0,_c1..._cn it will apply as column headers, if we use toDF(colnames) we can define our own headers..\n",
    "csv_df2=spark.read.csv(\"/Volumes/workspace/wd36schema/ingestion_volume/source/custs_1\").toDF(\"id\",\"fname\",\"lname\",\"age\",\"prof\")\n",
    "#csv_df1.show(2)#display with produce output in a dataframe format\n",
    "print(csv_df2.printSchema())\n",
    "csv_df2.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "009a120e-864d-4a4d-9117-147d3664dbe4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#2. Printing Schema (equivalent to describe table)\n",
    "csv_df1.printSchema()\n",
    "csv_df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8319201-e6bd-473f-bc0d-ea42c8ac4086",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#3. Inferring Schema \n",
    "# (Performance Consideration: Use this function causiously because it scans the entire data by immediately evaluating and executing\n",
    "# hence, not good for large data or not good to use on the predefined schema dataset)\n",
    "#sample data\n",
    "#4004979,Tara,Drake,32,\n",
    "#4004980,Earl,Hahn,34,Human resources assistant\n",
    "#4004981,Don,Jones,THIRTY SIX,Lawyer\n",
    "csv_df1=spark.read.csv(\"/Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/directory_dropme/custs_2.txt\",inferSchema=True).toDF(\"id\",\"fname\",\"lname\",\"age\",\"prof\")\n",
    "csv_df1.where(\"id in (4004979,4004981)\").show(2)\n",
    "csv_df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e53ae0a5-c97f-4b89-a77a-1fbf22ff7faf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Performance Importance: Though inferSchema has to be used causiously, we can improve performance by using an option to reduce the data scanned for large data.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ee3059d-3a82-46ba-adaa-2cfeaf3753df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_df1=spark.read.csv(\"/Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/directory_dropme/custs_2.txt\",inferSchema=True,samplingRatio=.10).toDF(\"id\",\"fname\",\"lname\",\"age\",\"prof\")\n",
    "csv_df1.where(\"id in (4004979,4004981)\").show(2)\n",
    "csv_df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a196167-00e2-4fdc-bbdd-89038a897939",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#4. Using delimiter or seperator option\n",
    "csv_df1=spark.read.csv(\"dbfs:///Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/directory_dropme/cust_other_del.txt\",header=True,sep='~')\n",
    "csv_df1.show(2)\n",
    "csv_df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f462c9a4-cfb3-4486-80f3-39455e717851",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#5. Using different options to create dataframe with csv and other module... (2 methodologies (spark.read.inbuiltfunction or spark.read.format(anyfunction).load(\"path\")) with 3 ways of creating dataframes (pass parameters to the csv()/option/options))\n",
    "csv_df1=spark.read.csv(\"dbfs:///Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/directory_dropme/cust_other_del.txt\",inferSchema=True,header=True,sep='~')\n",
    "csv_df1.show(2)\n",
    "#or another way of creating dataframe (from any sources whether builtin or external)...\n",
    "#option can be used for 1 or 2 option...\n",
    "csv_df2=spark.read.option(\"header\",\"True\").option(\"inferSchema\",\"true\").option(\"sep\",\"~\").format(\"csv\").load(\"dbfs:///Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/directory_dropme/cust_other_del.txt\")\n",
    "csv_df2.show(2)\n",
    "#options can be used for multiple options in one function as a parameter...\n",
    "csv_df3=spark.read.options(header=\"True\",inferSchema=\"true\",sep=\"~\").format(\"csv\").load(\"dbfs:///Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/directory_dropme/cust_other_del.txt\")\n",
    "csv_df3.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab7357e4-dc93-46c3-afa3-b8d1268d855c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Generic way of read and load data into dataframe using fundamental options from built in sources (csv/orc/parquet/xml/json/table) (inferschema, header, sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3854a111-973f-44c3-a0de-eb21b8c7e4cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_df1=spark.read.csv(\"dbfs:///Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/directory_dropme/cust_other_del.txt\",inferSchema=True,header=True,sep='~')\n",
    "csv_df1.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b7f4a41-0b3d-4e84-9349-48305109355a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Generic way of read and load data into dataframe using extended options from external sources (bigquery/redshift/athena/synapse) (tmpfolder, access controls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52edb76c-a8e0-44ec-889d-421b73cf1996",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#options can be used for multiple options in one function as a parameter...\n",
    "csv_df3=spark.read.options(header=\"True\",inferSchema=\"true\",sep=\"~\").format(\"csv\").load(\"dbfs:///Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/directory_dropme/cust_other_del.txt\")\n",
    "csv_df3.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "058e11df-a6de-4acf-8fc4-162b237b0f0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Reading data from multiple files & Multiple Path (We still have few more options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14f5f198-1d9f-4803-8bb8-35690e072b72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_df1=spark.read.csv(\"dbfs:///Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/directory_dropme/cust*\",inferSchema=True,header=True,sep='~')\n",
    "print(csv_df1.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72c96c04-b25a-40af-83ac-2b9e1aaed613",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_df1=spark.read.csv(path=[\"dbfs:///Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/directory_dropme/cust*\",\"/Volumes/workspace/wd36schema/ingestion_volume/source/custs_1\"],inferSchema=True,header=True,sep=',')\n",
    "print(csv_df1.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "711fe688-842d-4d1a-88f6-5c345747ea4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "RDD=spark.read.format(\"csv\").load(path=[\"/Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/\",\"/Volumes/workspace/default/usage_metrics\"],inferSchema=True,header=False,sep='~',pathGlobFilter=\"vij*\",recursiveFileLookup=True).toDF(\"ID\",\"fn\",\"ln\",\"aGE\",\"pROFESSION\")\n",
    "display(RDD)\n",
    "print(RDD.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "982272bb-381a-4fff-9f9a-636b6a9a74f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Requirement: I am getting data from different source systems of different regions (NY, TX, CA) into different landing pad (locations), how to access this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc5e8c61-eec9-4d7a-b560-18b81004af39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_multiple_sources=spark.read.csv(path=[\"/Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/sourcedata/NY\",\"dbfs:///Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/sourcedata/TX\"],inferSchema=True,header=True,sep=',',pathGlobFilter=\"custs_header_*\",recursiveFileLookup=True)\n",
    "#.toDF(\"cid\",\"fn\",\"ln\",\"a\",\"p\")\n",
    "print(df_multiple_sources.count())\n",
    "df_multiple_sources.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eccae843-0a73-4526-ae12-2349f4a64533",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Provide schema with SQL String or programatically (very very important)\n",
    "[PySpark SQL Datatypes](https://spark.apache.org/docs/latest/sql-ref-datatypes.html) <br>\n",
    "[Data Types](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/data_types.html)<br>\n",
    "###To provide schema (columname & datatype), what are the 2 basic options available that we learned so far ? inferSchema/toDF<br>\n",
    "###We are going to learn additionally 2 more options to handle schema (colname & datatype)?<br>\n",
    "###1. Using simple string format of define schema.<br>\n",
    "###IMPORTANT: 2. Using structure type to define schema.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f74d9fa-cf4b-473c-9a9c-0e623856c9d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#By default it will use _c0,_c1..._cn it will apply as column headers, if we use toDF(colnames) we can define our own headers..\n",
    "\n",
    "csv_df1=spark.read.csv(\"/Volumes/workspace/wd36schema/ingestion_volume/source/custs_1\").toDF(\"id\",\"fname\",\"lname\",\"age\",\"prof\")\n",
    "print(csv_df1.printSchema())\n",
    "csv_df1=spark.read.csv(\"/Volumes/workspace/wd36schema/ingestion_volume/source/custs_1\",inferSchema=True).toDF(\"id\",\"fname\",\"lname\",\"age\",\"prof\")\n",
    "print(csv_df1.printSchema())\n",
    "\n",
    "#1. Using simple string format of define custom simple schema.\n",
    "str_struct=\"id integer,fname string,lname string,age integer,prof string\"\n",
    "csv_df1=spark.read.schema(str_struct).csv(\"/Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/sourcedata/custs_header_1\")\n",
    "print(csv_df1.printSchema())\n",
    "csv_df1.show(2)\n",
    "\n",
    "#2. Important part - Using structure type to define custom complex schema.\n",
    "#4000001,Kristina,Chung,55,Pilot\n",
    "#pattern - \n",
    "#import the types library based classes..\n",
    "# define_structure=StructType([StructField(\"colname\",DataType(),True),StructField(\"colname\",DataType(),True)...])\n",
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType\n",
    "custom_schema=StructType([StructField(\"id\",IntegerType(),False),StructField(\"fname\",StringType(),True),StructField(\"lname\",StringType(),True),StructField(\"age\",IntegerType(),True),StructField(\"prof\",StringType())])\n",
    "csv_df1=spark.read.schema(custom_schema).csv(\"/Volumes/catalog1_dropme/schema1_dropme/volume1_dropme/sourcedata/custs_header_1\")\n",
    "print(csv_df1.printSchema())\n",
    "csv_df1.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ca48da7-bbbb-4710-aa50-f648e7c7b7cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "what will happen if one of the file have an extra column, rest all the columns are same?<br>\n",
    "Is union operation performing here in dataframe? not directly, but we can do to answer above question.. unionByName (Schema evolution)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "738627ae-187f-4a94-900c-a3c66a2bbcdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "What are all the overall options we used in this notebook, for learning fundamental spark csv read operations?"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6732402053954246,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) 1-Basic-Readops",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
